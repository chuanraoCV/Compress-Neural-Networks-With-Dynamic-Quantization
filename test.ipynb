{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import  tensorflow as tf\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def cifar10_input_stream(records_path):\n",
    "  reader = tf.TFRecordReader()\n",
    "  filename_queue = tf.train.string_input_producer([records_path], None)\n",
    "  _, record_value = reader.read(filename_queue)\n",
    "  features = tf.parse_single_example(record_value,\n",
    "    {\n",
    "      'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "      'label': tf.FixedLenFeature([], tf.int64),\n",
    "    })\n",
    "  image = tf.decode_raw(features['image_raw'], tf.uint8)\n",
    "  image = tf.reshape(image, [32,32,3])\n",
    "  image = tf.cast(image, tf.float32)\n",
    "  label = tf.cast(features['label'], tf.int64)\n",
    "  return image, label\n",
    "\n",
    "def normalize_image(image):\n",
    "  mean= [ 125.30690002,122.95014954,113.86599731]\n",
    "  std = [ 62.9932518,62.08860397,66.70500946]\n",
    "  normed_image = (image - mean) / std\n",
    "  return normed_image\n",
    "\n",
    "def random_distort_image(image):\n",
    "  distorted_image = image\n",
    "  distorted_image = tf.image.pad_to_bounding_box(image, 4, 4, 40, 40)  # pad 4 pixels to each side\n",
    "  distorted_image = tf.random_crop(distorted_image, [32, 32, 3])\n",
    "  distorted_image = tf.image.random_flip_left_right(distorted_image)\n",
    "  return distorted_image\n",
    "\n",
    "def make_train_batch(train_records_path, batch_size):\n",
    "  train_image, train_label = cifar10_input_stream(train_records_path)\n",
    "  train_image = normalize_image(train_image)\n",
    "  train_image = random_distort_image(train_image)\n",
    "  train_image_batch, train_label_batch = tf.train.shuffle_batch([train_image, train_label], batch_size=batch_size, num_threads=4,capacity=50000,min_after_dequeue=1000)\n",
    "  return train_image_batch, train_label_batch\n",
    "\n",
    "def make_validation_batch(test_records_path, batch_size):\n",
    "  test_image, test_label = cifar10_input_stream(test_records_path)\n",
    "  test_image = normalize_image(test_image)\n",
    "  test_image_batch, test_label_batch = tf.train.batch(\n",
    "    [test_image, test_label], batch_size=batch_size, num_threads=1,\n",
    "    capacity=10000)\n",
    "  return test_image_batch, test_label_batch\n",
    "\n",
    "\n",
    "def one_hot_embedding(label, n_classes):\n",
    "  \"\"\"\n",
    "  One-hot embedding\n",
    "  Args:\n",
    "    label: int32 tensor [B]\n",
    "    n_classes: int32, number of classes\n",
    "  Return:\n",
    "    embedding: tensor [B x n_classes]\n",
    "  \"\"\"\n",
    "  embedding_params = np.eye(n_classes, dtype=np.float32)\n",
    "  params = tf.constant(embedding_params)\n",
    "  embedding = tf.gather(params, label)\n",
    "  return embedding\n",
    "\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.01, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def conv2d(input, in_features, out_features, kernel_size, stride):\n",
    "    W = weight_variable([kernel_size, kernel_size, in_features, out_features])\n",
    "    return tf.nn.conv2d(input, W, [1, stride, stride, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def basic_block(input, in_features, out_features, stride, phase_train):\n",
    "    if stride == 1:\n",
    "        shortcut = input\n",
    "    else:\n",
    "        shortcut = tf.nn.avg_pool(input, [1, stride, stride, 1], [1, stride, stride, 1], 'VALID')\n",
    "        shortcut = tf.pad(shortcut, [[0, 0], [0, 0], [0, 0],[(out_features - in_features) // 2, (out_features - in_features) // 2]])\n",
    "    current = conv2d(input, in_features, out_features, 3, stride)\n",
    "    current = tf.contrib.layers.batch_norm(current, scale=True, is_training=phase_train, updates_collections=None)\n",
    "    current = tf.nn.relu(current)\n",
    "    current = conv2d(current, out_features, out_features, 3, 1)\n",
    "    current = tf.contrib.layers.batch_norm(current, scale=True, is_training=phase_train, updates_collections=None)\n",
    "    # No final relu as per http://torch.ch/blog/2016/02/04/resnets.html\n",
    "    return current + shortcut\n",
    "\n",
    "\n",
    "def block_stack(input, in_features, out_features, stride, depth, phase_train):\n",
    "    current = basic_block(input, in_features, out_features, stride, phase_train)\n",
    "    for _d in range(depth - 1):\n",
    "        current = basic_block(current, out_features, out_features, 1, phase_train)\n",
    "    return current\n",
    "\n",
    "phase_train = tf.placeholder(tf.bool, name='phase_train')\n",
    "learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "train_image_batch, train_label_batch = make_train_batch('F:/raochuan_code/my_ResNet_v2/data/data/train.tf', batch_size = 128)\n",
    "val_image_batch, val_label_batch = make_validation_batch('F:/raochuan_code/my_ResNet_v2/data/data/test.tf', batch_size = 100)\n",
    "image_batch, label_batch = control_flow_ops.cond(phase_train,lambda: (train_image_batch, train_label_batch),lambda: (val_image_batch, val_label_batch))\n",
    "# logits = residual_net(image_batch, 3, 10, phase_train)\n",
    "targets = one_hot_embedding(label_batch, 10)\n",
    "\n",
    "\n",
    "\n",
    "current = conv2d(image_batch, 3, 16, 3, 1)\n",
    "current = tf.nn.relu(current)\n",
    "\n",
    "# dimension is 32x32x16\n",
    "current = block_stack(current, 16, 16, 1, 6, phase_train)\n",
    "current = block_stack(current, 16, 32, 2, 6, phase_train)\n",
    "# dimension is 16x16x32\n",
    "current = block_stack(current, 32, 64, 2, 6, phase_train)\n",
    "# dimension is 8x8x64\n",
    "\n",
    "current = tf.reduce_mean(current, reduction_indices=[1, 2], name=\"avg_pool\")\n",
    "final_dim = 64\n",
    "current = tf.reshape(current, [-1, final_dim])\n",
    "Wfc = weight_variable([final_dim, 10])\n",
    "bfc = bias_variable([10])\n",
    "ys_ = tf.nn.softmax(tf.matmul(current, Wfc) + bfc)\n",
    "\n",
    "cross_entropy = -tf.reduce_mean(targets * tf.log(ys_ + 1e-12))\n",
    "\n",
    "# entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(targets,logits),name='entropy_loss')\n",
    "# entropy_loss = tf.reduce_mean(targets*tf.log(logits+1e-12),name='entropy_loss')\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(ys_, 1), tf.argmax(targets, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "tf.train.start_queue_runners(sess=sess)\n",
    "\n",
    "for epoch in range(1,81):\n",
    "    lr = 0.1\n",
    "    if epoch == 30:lr=0.01\n",
    "    if epoch == 60:lr=0.001\n",
    "    for turn in range(500):\n",
    "        batch_res = sess.run([train_step, accuracy], feed_dict={phase_train: True, learning_rate: lr})\n",
    "\n",
    "    # print('Train accuracy = %f' % batch_res[1])\n",
    "    n_val_samples = 10000\n",
    "    val_batch_size = 100\n",
    "    n_val_batch = int(n_val_samples / val_batch_size)\n",
    "    val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "    val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "    val_losses = []\n",
    "    for i in range(n_val_batch):\n",
    "        fetches = [ys_, label_batch]\n",
    "        session_outputs = sess.run(fetches, feed_dict={phase_train: False})\n",
    "        val_logits[i * val_batch_size:(i + 1) * val_batch_size, :] = session_outputs[0]\n",
    "        val_labels[i * val_batch_size:(i + 1) * val_batch_size] = session_outputs[1]\n",
    "    pred_labels = np.argmax(val_logits, axis=1)\n",
    "    val_accuracy = np.count_nonzero(pred_labels == val_labels) / n_val_samples\n",
    "    print('第%d 次测试，训练准确率：%f，测试准确率：%f' % (epoch, batch_res[1], val_accuracy))\n",
    "# save_path = saver.save(sess,'./model/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
